{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importaciones y Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import models, layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicciónario para convertir los valores a categorías\n",
    "LABELS = {0: \"LowQuality\", 1: \"Average\", 2: \"HighQuality\"}\n",
    "BINS = [0, 4.5, 5.5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar ambos datasets y convertir la columna quality a categorías\n",
    "def cargar_datos(dataset):\n",
    "    ds = pd.read_csv(dataset, sep=\";\")\n",
    "    ds[\"quality_cat\"] = pd.cut(ds[\"quality\"], bins=BINS, labels=[0, 1, 2])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear una nueva columna con las categorías sin modificar los valores originales\n",
    "def categorizar(dataset):\n",
    "    dataset[\"quality_cat\"] = pd.cut(dataset[\"quality\"], bins=BINS, labels=[0, 1, 2])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para definir la variable dependiente en clasificación y regresión\n",
    "def dividir_x_y(df, tipo):\n",
    "    if tipo == \"categorico\":\n",
    "        x = df.drop([\"quality\", \"quality_cat\"], axis=1)\n",
    "        y = df[\"quality_cat\"]\n",
    "    else:\n",
    "        x = df.drop([\"quality\", \"quality_cat\"], axis=1)\n",
    "        y = df[\"quality\"]\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para normalizar los datos\n",
    "def normalizar(x, y):\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(x)\n",
    "    y_onehot = keras.utils.to_categorical(y)\n",
    "    return x_scaled, y_onehot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para configurar y ejecutar el modelo de clasificación\n",
    "def clasificacion(X_scaled, y_onehot, fun_act):\n",
    "    activation_fn = fun_act\n",
    "\n",
    "    # Crear el modelo de clasificación con 3 capas de 64, 32 neuronas y 1 neurona por cada clase de salida\n",
    "    classification_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation=activation_fn, input_shape=(X_scaled.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation=activation_fn),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compilar el modelo de clasificación con optimizador Adam, función de pérdidad Categorical Cross-Entropy, y de métrica de evaluación la exactitud\n",
    "    classification_model.compile(optimizer='adam',\n",
    "                                loss='categorical_crossentropy',\n",
    "                                metrics=['accuracy'])\n",
    "\n",
    "    # K-fold cross validation para el modelo de clasificación\n",
    "    k = 5\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    acc = []\n",
    "    loss = []\n",
    "\n",
    "    # Entrenar el modelo de clasificación con 5-fold cross validation\n",
    "    for train_index, val_index in kf.split(X_scaled):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y_onehot[train_index], y_onehot[val_index]\n",
    "\n",
    "        classification_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)  # type: ignore\n",
    "\n",
    "        val_predictions = classification_model.predict(X_val)\n",
    "        val_predictions = np.argmax(val_predictions, axis=1)\n",
    "        val_true_labels = np.argmax(y_val, axis=1)\n",
    "\n",
    "        acc.append(accuracy_score(val_true_labels, val_predictions))\n",
    "        loss.append(classification_model.evaluate(X_val, y_val, verbose=0)[0])  # type: ignore\n",
    "\n",
    "    # Se imprime el promedio de los valores retornados realizando la cross validation\n",
    "    print(\"\\nPrecision con función de activación \", fun_act, \": \", np.mean(acc))\n",
    "    print(\"Costo de perdida con función de activación \", fun_act, \": \", np.mean(loss))\n",
    "    return np.mean(acc), np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para configurar y ejecutar el modelo de regresión\n",
    "def regresion(X_scaled, y, fun_act):\n",
    "    activation_fn = fun_act\n",
    "\n",
    "    # Crear el modelo de regresión con 3 capas de 64, 32 neuronas y 1 neurona de salida\n",
    "    regression_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation=activation_fn, input_shape=(X_scaled.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation=activation_fn),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compilar el modelo de regresión con optimizador Adam, función de pérdidad Error cuadrático medio, y de métricas de evaluación el error absoluto y cuadrático medio \n",
    "    regression_model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "\n",
    "    # K-fold cross validation para el modelo de regresión\n",
    "    k = 5\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    mse_scores = []\n",
    "    mae_scores = []\n",
    "\n",
    "    # Entrenar el modelo de regresión con 5-fold cross validation\n",
    "    for train_index, val_index in kf.split(X_scaled):\n",
    "        X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        \n",
    "        regression_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0) # type: ignore\n",
    "        \n",
    "        val_predictions = regression_model.predict(X_val).flatten()\n",
    "        mse = mean_squared_error(y_val, val_predictions)\n",
    "        mae = mean_absolute_error(y_val, val_predictions)\n",
    "        mse_scores.append(mse)\n",
    "        mae_scores.append(mae)\n",
    "\n",
    "    # Se imprime el promedio de los valores retornados realizando la cross validation   \n",
    "    print(\"\\nMSE con función de activación \", fun_act, \": \", np.mean(mse_scores))\n",
    "    print(\"MAE con función de activación \", fun_act, \": \", np.mean(mae_scores))\n",
    "    return np.mean(mse_scores), np.mean(mae_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función entrega tablas comparativas con las funciones Sigmoide y Tangente hiperbólica\n",
    "def tabla_comparative(sigmoide, tanh, val1, val2):\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"Función de activación\": [val1, val2],\n",
    "            \"Sigmoid\": [sigmoide[0], sigmoide[1]],\n",
    "            \"Tanh\": [tanh[0], tanh[1]],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vino Blanco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vino_blanco = cargar_datos(\"Data/winequality-white.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de clasificacion usando Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dividir_x_y(vino_blanco, \"categorico\") # regresion o categorico\n",
    "x_scaled, y_onehot = normalizar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 764us/step\n",
      "31/31 [==============================] - 0s 813us/step\n",
      "31/31 [==============================] - 0s 804us/step\n",
      "31/31 [==============================] - 0s 938us/step\n",
      "31/31 [==============================] - 0s 972us/step\n",
      "\n",
      "Precision con función de activación  sigmoid :  0.7380590356673824\n",
      "Costo de perdida con función de activación  sigmoid :  0.5977986097335816\n"
     ]
    }
   ],
   "source": [
    "clas_sigmoide = clasificacion(x_scaled, y_onehot, \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 899us/step\n",
      "31/31 [==============================] - 0s 832us/step\n",
      "31/31 [==============================] - 0s 752us/step\n",
      "31/31 [==============================] - 0s 940us/step\n",
      "31/31 [==============================] - 0s 697us/step\n",
      "\n",
      "Precision con función de activación  tanh :  0.7574622167559567\n",
      "Costo de perdida con función de activación  tanh :  0.5454127728939057\n"
     ]
    }
   ],
   "source": [
    "clas_tanh = clasificacion(x_scaled, y_onehot, \"tanh\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de Regresion usando Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dividir_x_y(vino_blanco, \"regresion\") # regresion o categorico\n",
    "x_scaled, _ = normalizar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 817us/step\n",
      "31/31 [==============================] - 0s 833us/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 770us/step\n",
      "31/31 [==============================] - 0s 890us/step\n",
      "\n",
      "MSE con función de activación  sigmoid :  0.5564789959702159\n",
      "MAE con función de activación  sigmoid :  0.5790312366341862\n"
     ]
    }
   ],
   "source": [
    "reg_sigmoide = regresion(x_scaled, y, \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 950us/step\n",
      "31/31 [==============================] - 0s 830us/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "31/31 [==============================] - 0s 935us/step\n",
      "31/31 [==============================] - 0s 1ms/step\n",
      "\n",
      "MSE con función de activación  tanh :  0.5267068576794096\n",
      "MAE con función de activación  tanh :  0.5685421073940453\n"
     ]
    }
   ],
   "source": [
    "reg_tanh = regresion(x_scaled, y, \"tanh\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de los Resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Classification\n",
    "\n",
    "Para el modelo de clasificación de vinos blancos, podemos ver que los resultados son dentro de todo similares, teniendo en cuenta esto, la funcion tangente hiperbólica (tanh) presenta un menor costo de pérdida a comparanción de la función sigmoidal. Lo que nos lleva a concluir que la función tangente hiperbólica sería la mas adecuada a implementar en este modelo. Se puede observar que la exactitud (Accuracy) al utilizar la función tangente hiperbolica levemente mayor que la de función sigmoidal. Sin embargo, es importante considerar que esta métrica puede llegar a ser no la ideal al momento de realizar las evaluaciones ya que como sabemos gracias a los laboratorios anteriores, el dataset se encuentra desbalanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Función de activación</th>\n",
       "      <th>Sigmoid</th>\n",
       "      <th>Tanh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Precisión</td>\n",
       "      <td>0.738059</td>\n",
       "      <td>0.757462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Costo de perdida</td>\n",
       "      <td>0.597799</td>\n",
       "      <td>0.545413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Función de activación   Sigmoid      Tanh\n",
       "0             Precisión  0.738059  0.757462\n",
       "1      Costo de perdida  0.597799  0.545413"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla_comparative(clas_sigmoide, clas_tanh, \"Precisión\", \"Costo de perdida\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Regression\n",
    "Para el modelo de regresión de vinos blancos podemos ver que los errores (cuadrático y absoluto medio), son dentro de todo similares, teniendo en cuenta esto, la funcion tangente hiperbólica (tanh) presenta errores mas pequeños a comparanción de la función sigmoidal. Lo que nos lleva a concluir que la función de activación de la tangente hiperbólica sería la mas adecuada a implementar en este modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Función de activación</th>\n",
       "      <th>Sigmoid</th>\n",
       "      <th>Tanh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSE</td>\n",
       "      <td>0.556479</td>\n",
       "      <td>0.526707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAE</td>\n",
       "      <td>0.579031</td>\n",
       "      <td>0.568542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Función de activación   Sigmoid      Tanh\n",
       "0                   MSE  0.556479  0.526707\n",
       "1                   MAE  0.579031  0.568542"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla_comparative(reg_sigmoide, reg_tanh, \"MSE\", \"MAE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vino Tinto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vino_tinto = cargar_datos(\"Data/winequality-red.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de clasificacion usando Red Neuronal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dividir_x_y(vino_tinto, \"categorico\") # regresion o categorico\n",
    "x_scaled, y_onehot = normalizar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 945us/step\n",
      "10/10 [==============================] - 0s 947us/step\n",
      "10/10 [==============================] - 0s 979us/step\n",
      "10/10 [==============================] - 0s 820us/step\n",
      "10/10 [==============================] - 0s 918us/step\n",
      "\n",
      "Precision con función de activación  sigmoid :  0.7173315047021944\n",
      "Costo de perdida con función de activación  sigmoid :  0.6434007525444031\n"
     ]
    }
   ],
   "source": [
    "clas_sigmoide = clasificacion(x_scaled, y_onehot, \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 947us/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 787us/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 844us/step\n",
      "\n",
      "Precision con función de activación  tanh :  0.7360717084639499\n",
      "Costo de perdida con función de activación  tanh :  0.593756127357483\n"
     ]
    }
   ],
   "source": [
    "clas_tanh = clasificacion(x_scaled, y_onehot, \"tanh\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de Regresion usando Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = dividir_x_y(vino_tinto, \"regresion\") # regresion o categorico\n",
    "x_scaled, _ = normalizar(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 926us/step\n",
      "10/10 [==============================] - 0s 834us/step\n",
      "10/10 [==============================] - 0s 843us/step\n",
      "10/10 [==============================] - 0s 1000us/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "\n",
      "MSE con función de activación  sigmoid :  0.4754662273089897\n",
      "MAE con función de activación  sigmoid :  0.5370519954419248\n"
     ]
    }
   ],
   "source": [
    "reg_sigmoide = regresion(x_scaled, y, \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "10/10 [==============================] - 0s 895us/step\n",
      "10/10 [==============================] - 0s 756us/step\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "\n",
      "MSE con función de activación  tanh :  0.41264736424706844\n",
      "MAE con función de activación  tanh :  0.4973138504968168\n"
     ]
    }
   ],
   "source": [
    "reg_tanh = regresion(x_scaled, y, \"tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de los Resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Classification\n",
    "Para el modelo de clasificación de vinos tintos, al igual que con los vinos blancos, la funcion tangente hiperbólica (tanh) presenta un menor costo de pérdida a comparanción de la función sigmoidal. Lo que nos lleva a concluir que la función tangente hiperbólica sería la mas adecuada a implementar en este modelo. Se puede observar que la precisión (Accuracy) al utilizar la función tangente hiperbolica levemente mayor que la de función sigmoidal. Sin embargo, es importante considerar que esta métrica puede llegar a ser no la ideal al momento de realizar las evaluaciones ya que como sabemos gracias a los laboratorios anteriores, el dataset se encuentra desbalanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Función de activación</th>\n",
       "      <th>Sigmoid</th>\n",
       "      <th>Tanh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Precisión</td>\n",
       "      <td>0.717332</td>\n",
       "      <td>0.736072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Costo de perdida</td>\n",
       "      <td>0.643401</td>\n",
       "      <td>0.593756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Función de activación   Sigmoid      Tanh\n",
       "0             Precisión  0.717332  0.736072\n",
       "1      Costo de perdida  0.643401  0.593756"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla_comparative(clas_sigmoide, clas_tanh, \"Precisión\", \"Costo de perdida\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de Regression\n",
    "Para el modelo de regresión de vinos tintos, al igual que en los vinos blancos, podemos ver que los errores (cuadrático y absoluto medio), son dentro de todo similares (esta vez mas existe menor diferencia entre ellos comparando esa diferencia entre errores de los vinos blancos). La funcion tangente hiperbólica (tanh) presenta errores mas pequeños a comparanción de la función sigmoidal. Esto nos lleva a concluir que la función de activación de la tangente hiperbólica sería la mas adecuada a implementar en este modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Función de activación</th>\n",
       "      <th>Sigmoid</th>\n",
       "      <th>Tanh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSE</td>\n",
       "      <td>0.475466</td>\n",
       "      <td>0.412647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MAE</td>\n",
       "      <td>0.537052</td>\n",
       "      <td>0.497314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Función de activación   Sigmoid      Tanh\n",
       "0                   MSE  0.475466  0.412647\n",
       "1                   MAE  0.537052  0.497314"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla_comparative(reg_sigmoide, reg_tanh, \"MSE\", \"MAE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
